
<h1 align="center">
  <p>WorldRWKV: Exploring RWKV7â€™s Understanding Capabilities of Any Modality in the World</p>
  
</h1>

\[ English | [ä¸­æ–‡](README_zh.md) \]
## ç®€ä»‹
ä½¿ç”¨åŸç”ŸRWKV7å®ç°ä»»æ„æ¨¡æ€çš„è¾“å…¥è¾“å‡ºï¼Œæœç€World Modelè¿›å‘
## å‘å¸ƒ
- [3/7] ğŸ”¥ å¼€æºä»“åº“ **WorldRWKV: Exploring RWKV7â€™s Understanding Capabilities of Any Modality in the World**. é¢„è®¡ä¸‹å‘¨å‘å¸ƒè®ºæ–‡å’Œè®­ç»ƒç»†èŠ‚ [HFModel](https://huggingface.co/WorldRWKV).
## ç¯å¢ƒ
- å…‹éš†ä»“åº“å¹¶è¿›å…¥æ–‡ä»¶
```
git clone https://github.com/JL-er/WorldRWKV.git
cd WorldRWKV
```
- å®‰è£…åŒ…
```
conda create -n world python=3.12
conda activate world
pip install -r requirements.txt #ä¸­å›½ç”¨æˆ·æ·»åŠ -i https://pypi.tuna.tsinghua.edu.cn/simple
# æ¨è torch=>2.4.0
```
## æ¨ç†
> [!NOTE]
> ä½¿ç”¨çš„encoderæ¨¡å‹éœ€è¦å’Œencoder_typeå¯¹åº”,å…·ä½“å†…å®¹è¯·åœ¨world/world_encoder.pyä¸­æŸ¥çœ‹
```
from infer.worldmodel import Worldinfer
from PIL import Image


llm_path='/home/rwkv/model/rwkv7-3b-siglip/rwkv-0'
encoder_path='/home/rwkv/model/siglip2basep16s384'
encoder_type='siglip' #[clip, whisper, siglip, speech]

model = Worldinfer(model_path=llm_path, encoder_type=encoder_type, encoder_path=encoder_path)

img_path = './docs/03-Confusing-Pictures.jpg'
image = Image.open(img_path).convert('RGB')

text = '\x16User: What is unusual about this image?\x17Assistant:'

result = model.generate(text, image)

print(result)
```

## è®­ç»ƒ
> [!NOTE]
> ä½¿ç”¨çš„encoderæ¨¡å‹éœ€è¦å’Œencoder_typeå¯¹åº”,ä¸åŒä»»åŠ¡éœ€è¦æœ‰å¯¹åº”çš„data_typeã€‚ä½ ä¹Ÿå¯ä»¥åœ¨world/world_encoder.pyä¸­åˆ›å»ºè‡ªå·±çš„æ¨¡æ€ç±»
```
load_model=/home/rwkvos/model/rwkv/RWKV-x070-World-2.9B-v3-20250211-ctx4096.pth
proj_dir=/home/rwkvos/peter/out_model/rwkv7-3b-pretrain-siglip
data_file=/home/rwkvos/data/hf-imgs/pretrain595

n_layer=32
n_embd=2560

encoder_path="google/siglip2-base-patch16-384" #é€‰æ‹©ä½ éœ€è¦çš„encoder
encoder_type=siglip #åœ¨worldencoderä¸­æ³¨å†Œç±»å‹
data_type=hf_img #æ•°æ®ç±»å‹

micro_bsz=32
epoch_save=1
epoch_steps=18605 
ctx_len=2048


HF_ENDPOINT="https://hf-mirror.com" python world_train.py \   # ä¸­å›½ç”¨æˆ·ä½¿ç”¨"https://hf-mirror.com"ä¸‹è½½æ¨¡å‹
--load_model $load_model \
--proj_dir $proj_dir --data_file $data_file \
--data_type $data_type \
--vocab_size 65536 \
--n_layer $n_layer --n_embd $n_embd \
--ctx_len $ctx_len --micro_bsz $micro_bsz \
--epoch_steps $epoch_steps --epoch_count 1 --epoch_begin 0 --epoch_save $epoch_save \
--lr_init 1e-3 --lr_final 0 --warmup_steps 0 --beta1 0.9 --beta2 0.99 --adam_eps 1e-8 \
--accelerator gpu --devices 8 --precision bf16 --strategy deepspeed_stage_1 --grad_cp 1 \
--encoder_path $encoder_path --encoder_type $encoder_type \
--my_testing "x070" --train_step adapter rwkv #train_step é€‰æ‹©ä½ è¦è®­ç»ƒçš„éƒ¨åˆ†ï¼Œadapterã€rwkv
```

## åŠŸèƒ½
### WorldRWKVå·²å®ç°çš„åŠŸèƒ½ä»¥åŠåç»­æ·»åŠ çš„åŠŸèƒ½
| Function      | Work |
|:--------------:|:-----------:|
| asr            | âœ…          |
| speech to text | âœ…          |
| visual to text | âœ…          |
| text to speech | âŒ          |
| text to visual | âŒ          |
|speech to speech| âŒ          |


## è§†è§‰é—®ç­”åŸºå‡†

| **Encoder** | **LLM** | **VQAV2** | **TextVQA** | **GQA** | **ScienceQA** | **Checkpoint** |
|:--------------:|:--------------:|:--------------:|:--------------:|:--------------:|:--------------:|:--------------:|
| [**Clip**](https://huggingface.co/openai/clip-vit-large-patch14-336)    | RWKV7-0.4B     | 62.04      | 31.72      | 49.32       |   51.10         |
|| RWKV7-1.5B     | 72.31       | 40.27       | 54.56       |   62.77          |
|             | RWKV7-3B       | 73.13       | 45.56       | 57.00       | 70.06       |
| [**SigLIP2**](https://huggingface.co/google/siglip2-base-patch16-384) | RWKV7-0.4B|    72.04     | 38.75       | 55.52       | 43.32       |[WorldRWKV/RWKV7-0.4B-siglip2](https://huggingface.co/WorldRWKV/RWKV7-0.4B-siglip2)     |
|             | RWKV7-1.5B   |     76.95    | 44.96       | 58.88       | 63.10       |[WorldRWKV/RWKV7-1.5B-siglip2](https://huggingface.co/WorldRWKV/RWKV7-1.5B-siglip2)     |
|             | RWKV7-3B      |     78.30     |   51.09          |   60.75          |     70.93        |[WorldRWKV/RWKV7-3B-siglip2](https://huggingface.co/WorldRWKV/RWKV7-3B-siglip2)       |

## è¯­éŸ³è¯†åˆ«åŸºå‡†

| **Encoder** | **LLM** | **LibriSpeech** | **Aishell-1** |
|:--------------:|:--------------:|:--------------:|:--------------:|
|[**wavlm large**](https://huggingface.co/microsoft/wavlm-large) | RWKV7-0.4B | 2.43%(clean) | 9.68%(dev) |
|            |            | 6.51%(other) | 10.33%(test) |
|[**wavlm base+**](https://huggingface.co/microsoft/wavlm-base-plus) | RWKV7-0.4B | 3.08%(clean) | 12.40%(dev) |
|            |            | 10.38%(other) | 13.46%(test) |

## è¯­éŸ³è¯†åˆ«&è¯­éŸ³é—®ç­”(Demo)
| **Encoder** | **LLM** | **task** | **Checkpoint** |
|:--------------:|:--------------:|:--------------:|:--------------:|
|[**wavlm large**](https://huggingface.co/microsoft/wavlm-large) | RWKV7-0.1B | EN asr|[WorldRWKV/RWKV7-0.1B-wavlmLarge-ENASR-demo](https://huggingface.co/WorldRWKV/RWKV7-0.1B-wavlmLarge-ENASR-demo)|
|            |     RWKV7-0.4B       | EN asr|[WorldRWKV/RWKV7-0.4B-wavlmLarge-ENASR-demo](https://huggingface.co/WorldRWKV/RWKV7-0.4B-wavlmLarge-ENASR-demo)|
|            |     RWKV7-0.4B       | CN asr|[WorldRWKV/RWKV7-0.4B-wavlmLarge-CNASR-demo](https://huggingface.co/WorldRWKV/RWKV7-0.4B-wavlmLarge-CNASR-demo)|
|            |     RWKV7-0.4B       | EN qa|[WorldRWKV/RWKV7-0.4B-wavlmLarge-ENQA-demo](https://huggingface.co/WorldRWKV/RWKV7-0.4B-wavlmLarge-ENQA-demo)|


